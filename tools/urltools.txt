httpx (Prober) - To filter live sites.

Waymore (Deep History) - To find old URLs from Wayback Machine.

Gau (Fast History) - To find URLs from AlienVault/CommonCrawl.

ParamSpider (Mining) - To find URLs specifically with parameters (e.g., ?id=).

Katana (Active Crawler) - To browse sites with a headless browser.

Hakrawler (Fast Crawler) - To cross-check Katana quickly.

GitDorker (GitHub Leaks) - To find internal URLs leaked in code.

Ffuf (Brute Force) - To guess hidden folders.

JSluice (Deep JS) - To analyze JavaScript structure.

XnLinkFinder (JS Regex) - To extract endpoints from JS text.

Installation Commands (For Jules):

Bash

go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest
pip install waymore
go install -v github.com/lc/gau/v2/cmd/gau@latest
git clone https://github.com/devanshbatham/ParamSpider tools/ParamSpider && pip install -r tools/ParamSpider/requirements.txt
go install -v github.com/projectdiscovery/katana/cmd/katana@latest
go install -v github.com/hakluke/hakrawler@latest
git clone https://github.com/obheda12/GitDorker.git tools/GitDorker && pip install -r tools/GitDorker/requirements.txt
go install -v github.com/ffuf/ffuf/v2@latest
go install -v github.com/bishopfox/jsluice/cmd/jsluice@latest
go install -v github.com/xnl-h4ck3r/xnLinkFinder@latest
go install -v github.com/tomnomnom/anew@latest
2. Strategic Instructions (The "Brain" Logic)
Input: output/activesubdomain.txt (List of domains).

Step A: The Filter (Probing)
Tool: httpx

Logic: Do not scan dead domains. Check ports 80, 443, 8080, 8443.

AI Decision: If a domain returns a 403 Forbidden or 401 Unauthorized, do not discard it. These are often Admin panels that are "live" but locked. Keep them.

Output: Create a live_seeds.txt list.

Step B: History Mining (The Time Machine)
Tools: Waymore + Gau

Logic: Developers often delete vulnerable pages but forget to remove them from archives.

Instruction: Run Waymore for deep fetches (limit to last 2 years to save time). Run Gau for broad fetches.

Output: Save to raw_history.txt.

Step C: Parameter Hunting (The Miner)
Tool: ParamSpider

Logic: We specifically want URLs that accept input (parameters), as these are attackable.

Instruction: Run ParamSpider on the domains. It will automatically ignore static images and focus on ?id=, ?user=, etc.

Step D: Active Crawling (The Spider)
Tools: Katana + Hakrawler

Logic: Visit the live sites now.

AI Decision: Use Katana with JavaScript enabled (-jc). If Katana gets stuck or takes too long on a specific domain (e.g., > 10 mins), kill it and switch to Hakrawler for that domain.

Step E: Source Code Recon (The Leak Hunter)
Tool: GitDorker (Requires GitHub Token)

Logic: Developers accidentally push internal links to GitHub.

Instruction: Search GitHub for the domain name + keywords like "api", "secret", "admin".

AI Note: If you don't have a GitHub token configured, skip this tool, but note in the logs that Source Code recon was skipped.

Step F: Brute Force (The Guesser)
Tool: Ffuf

Logic: Crawlers can't see what isn't linked. You must guess.

Instruction: Use a "Common Web Content" wordlist. Fuzz for hidden directories (e.g., /.git, /backup, /api).

AI Decision: Monitor response codes. If you see many 429 (Rate Limit) errors, automatically slow down the scan.

Step G: Deep JS Analysis (The Surgeon)
Tools: JSluice + XnLinkFinder

Logic: Modern apps (React/Vue) hide all their logic in .js files.

Instruction:

Filter your current URL list to find all .js files.

Run JSluice to extract secrets (AWS keys) and endpoints from within the code structure.

Run XnLinkFinder as a backup to regex-match any strings that look like URLs.

3. The AI Analysis Task (Categorization)
Tell Jules to stop and process the data before finishing.

Instruction: "Jules, now that you have collected thousands of URLs from these 10 sources, merge them, remove duplicates, and intelligently sort them into these 5 priority buckets:"

endpoints.txt -> High Priority. Any URL with parameters (?id=) or dynamic extensions (.php, .jsp). These will be used for SQLi/XSS testing.

login_panels.txt -> High Priority. Any URL containing "admin", "login", "dashboard", "signin". These will be used for Auth testing.

cloud_buckets.txt -> Medium Priority. URLs pointing to S3, Azure, or Google Cloud Storage. These will be checked for public access.

sensitive_files.txt -> Critical Priority. URLs ending in .env, .json, .xml, .log. These often contain passwords.

static_assets.txt -> Ignore. Images, CSS, Fonts. Separate these so we don't waste time scanning them.