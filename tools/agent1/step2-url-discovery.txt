httpx (Prober) - To filter live sites.

Waymore (Deep History) - To find old URLs from Wayback Machine.

Gau (Fast History) - To find URLs from AlienVault/CommonCrawl.

ParamSpider (Mining) - To find URLs specifically with parameters (e.g., ?id=).

Katana (Active Crawler) - To crawl live sites and discover endpoints (incl. JS parsing modes).

Hakrawler (Fast Crawler) - To cross-check Katana quickly.

GitDorker (GitHub Leaks) - To find internal URLs leaked in code.

Ffuf (Brute Force) - To guess hidden folders.

Naabu (Ports) - To find open ports/services beyond 80/443.

Nmap (Validate) - To validate and fingerprint ports/services (external install).

TLSX (TLS/Cert Intel) - To collect certificate SANs + TLS hints.

Gowitness (Screenshots) - To quickly triage what is actually running.

Subjack (Takeover candidates) - To detect potential subdomain takeover indicators (scope-aware).


Wafw00f (WAF Detection) - To detect WAF/CDN protections so scan speed can be adjusted.

NOTE: Step split (important)
- Step 2 = COLLECTION + ARTIFACT CAPTURE + MERGE (build the best corpus).
- Step 3 = ANALYSIS + PRIORITIZED HEAVY SCANS + BUCKETING.
- Step 3 instructions:
	- tools/agent1/step3-analysis-and-prioritized-scanning.txt

Installation Commands (For Jules)

# Go tools
go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest
go install -v github.com/lc/gau/v2/cmd/gau@latest
go install -v github.com/projectdiscovery/katana/cmd/katana@latest
go install -v github.com/hakluke/hakrawler@latest
go install -v github.com/ffuf/ffuf/v2@latest
go install -v github.com/projectdiscovery/naabu/v2/cmd/naabu@latest
go install -v github.com/projectdiscovery/tlsx/cmd/tlsx@latest
go install -v github.com/sensepost/gowitness@latest
go install -v github.com/haccer/subjack@latest
go install -v github.com/tomnomnom/anew@latest

# Recommended (quality): URL normalization helpers
# - unfurl helps extract hosts/paths/params cleanly (great for allowlisting)
go install -v github.com/tomnomnom/unfurl@latest
# - uro normalizes URLs and reduces duplicates (great before final merge)
pip install uro

# Python tools
pip install waymore

# WAF detection (recommended before heavy active scans)
pip install wafw00f

# ParamSpider
git clone https://github.com/devanshbatham/ParamSpider tools/ParamSpider
pip install -r tools/ParamSpider/requirements.txt

# GitDorker
git clone https://github.com/obheda12/GitDorker.git tools/GitDorker
pip install -r tools/GitDorker/requirements.txt


2. Strategic Instructions (The "Brain" Logic)
Input: outputs/activesubdomain.txt (List of subdomains).

Core rule:
- Stay in-scope. Do not follow links to out-of-scope domains.

Before running:
- Ensure folders exist: temp/agent1 and outputs

Note:
- The large ffuf content wordlist and Kiterunner routes wordlist are used in Step 3.
- Step 2 only requires the small known-files/API-docs wordlist for Step A3.

External files / credentials (download + place)

1) Known-files + API-docs micro wordlist (required for Step A3)
- You can create this manually (recommended) as a small list of high-signal paths:
	- robots.txt
	- sitemap.xml
	- sitemap_index.xml
	- .well-known/security.txt
	- security.txt
	- humans.txt
	- swagger
	- swagger-ui
	- swagger-ui.html
	- api-docs
	- v2/api-docs
	- v3/api-docs
	- openapi.json
	- swagger.json
	- redoc
- Save as: temp/agent1/knownfiles_and_apidocs.txt

Wordlist strategy (Step A3):
- Keep this list small and high-signal.
- Its job is to quickly find robots/sitemaps/security.txt and Swagger/OpenAPI docs.

2) GitHub token (optional but recommended for Step E: GitDorker)
- Create a token with appropriate permissions for search.
- Provide to the tool via its expected config/env var mechanism.
- If no token is available, skip GitDorker and log that it was skipped.

Scope enforcement rule (important):
- Build an allowlist of hosts from outputs/activesubdomain.txt
- When collecting URLs (history/crawl/js/ffuf/git), DROP any URL whose host is not in that allowlist
	(This prevents the pipeline from drifting into third-party SaaS/CDN domains)

Normalization rule (important):
- Before final output, normalize URLs (remove obvious dupes, standardize)
- Use uro on URL lists whenever you merge sources

# File contract (Step 2)
# Input:
# - outputs/activesubdomain.txt (from step 1)
#
# Intermediates written to temp/agent1/:
# - ports_raw.txt
# - ports_open.txt
# - web_port_targets.txt
# - tlsx.txt
# - screenshots/ (gowitness output)
# - takeover_candidates_raw.txt
# - cnames.txt
# - live_seeds.txt
# - live_base_urls.txt
# - waf_results.txt
# - host_fingerprints.txt
# - known_files_hits.txt
# - api_docs_raw/ (downloaded swagger/openapi files)
# - raw_history.txt
# - url_corpus_all_in_scope.txt (merged + deduped, in-scope only)
# - html_responses/ (saved HTML responses)
#
# (Produced in Step 3, not Step 2)
# - See: tools/agent1/step3-analysis-and-prioritized-scanning.txt
#
# Step 2 outputs (write to outputs/):
# - outputs/coverage_open_ports_hostport.txt
# - outputs/coverage_non_http_services.txt
# - outputs/coverage_tls_sans_in_scope.txt
# - outputs/coverage_takeover_candidates_subjack.txt
# - outputs/coverage_screenshots_index_gowitness.txt
# - outputs/reports/agent1-recon-report.md (running report / decisions)
# - outputs/api_docs_urls.txt
# - outputs/api_endpoints_from_openapi.txt
# - outputs/web_knownfiles_robots_sitemaps_security_urls.txt

# Step 3 outputs (created later; see Step 3 doc)
# - See: tools/agent1/step3-analysis-and-prioritized-scanning.txt

Reporting (Mandatory)
Goal:
- Save the agent’s analysis decisions and key findings in ONE place for later vulnerability testing.

Instruction:
- Maintain a running report file during Step 2:
	- outputs/reports/agent1-recon-report.md
- At every Brain Stop in Step 2 (#0, #2–#5), append a short section:
	- What you observed (counts + notable responses)
	- What you decided (rate/parallelism, which hosts to prioritize)
	- What you will do next (next steps + which output files matter)

Artifact retention (Mandatory)
- Do NOT delete the temp folder after the run.
- Keep all artifacts in: temp/agent1/
	- This includes: html_responses/, host_fingerprints.txt, waf_results.txt, raw_history.txt, api_docs_raw/, url_corpus_all_in_scope.txt, etc.
- If disk space is an issue, compress/archive temp/agent1/ instead of removing it.

Error / Skip Reporting (Mandatory)
Goal:
- If a tool fails (dependency, permissions, timeouts, command not found) you must record it cleanly so the run is reproducible and fixable.

Instruction:
- Maintain a separate error/skip report during Step 2:
	- outputs/reports/agent1-error-report.md

When to write an entry:
- Tool could not execute (missing dependency / command not found)
- Tool executed but produced no output due to errors/timeouts
- Tool was intentionally skipped (time, token missing, WAF risk)

Required fields per entry (copy/paste template):

### <Tool Name>
- Status: failed | skipped | partial
- Step: (e.g., Step 0A, Step A1b, Step J)
- Command(s):
	- <exact command you ran>
- Error (copy exact message):
	- <stderr/stdout snippet>
- Reason (one sentence):
	- <why it failed or why you skipped>
- Impact:
	- <what coverage is missing / what outputs will be absent>
- Mitigation / Next fix:
	- <what to install/change next time>

Rules:
- Do not write secrets/tokens into the report (redact values).
- If a host-specific failure occurs (SSL error/timeouts), list the affected hosts.
- If you skip a tool for time, state what you used instead (fallback).

Step 0: Coverage Checks (Mandatory before web vuln testing)
Goal:
- Ensure you do NOT miss exposure on non-default ports, non-HTTP services, TLS/cert hints, and takeover candidates.

Core rule:
- Stay in-scope. Do not scan or follow redirects to out-of-scope hosts.

Step 0A: Port discovery (Mandatory)
Tools: naabu (+ optional nmap)

Instruction:
- Run naabu on outputs/activesubdomain.txt to find open ports.
- Keep it conservative (rate/parallelism) by default.
	- After Brain Stop #2 (Post-Probe Decisions), further reduce rates if WAF/rate limits are observed.
- Save raw + cleaned lists:
	- temp/agent1/ports_raw.txt
	- temp/agent1/ports_open.txt (host:port)

Troubleshooting (common failure: pcap/libpcap)
- If naabu fails with a libpcap/pcap.h error, you have two options:
	1) Install the missing system dependency (typical on Linux).
	2) Use a TCP connect scan mode if your naabu build supports it (often avoids pcap requirements).
- Always run `naabu -h` to confirm the exact flag names for your version.

Outputs:
- outputs/coverage_open_ports_hostport.txt (clean, unique host:port)
- outputs/coverage_non_http_services.txt (anything clearly not web: ssh, smtp, rdp, db ports)

Brain Stop #0 (Mandatory): Port Triage (Think like a hacker)
Goal:
- Don’t treat port scanning as a checkbox.
- Use discovered ports to decide what to probe and what to manually investigate.

Instruction to Jules:
- STOP running tools.
- Review outputs/coverage_open_ports_hostport.txt and outputs/coverage_non_http_services.txt.
- For any unusual/high-value ports you find, decide a targeted next action, for example:
	- If port looks like HTTP/admin panels (any non-standard port): ensure it is included in Step 0A1 probing.
	- If port looks like a service (e.g., 22/25/445/6379/9200/27017/etc): prioritize validation with nmap and add a manual testing note.
- Append a short “Port triage” note (which ports matter and why) to: outputs/reports/agent1-recon-report.md

Step 0A1: Probe web on ALL open ports (Mandatory)
Why:
- If you discovered any non-default ports (e.g., 9100/8080/8443/3000/etc), you MUST probe them.
- Default-only probing misses panels/services running on other ports.

Instruction:
- Take outputs/coverage_open_ports_hostport.txt (host:port) and build a URL target list:
	- For 443: use https://HOST
	- For 80: use http://HOST
	- For any other port (e.g., 9100): try BOTH https://HOST:PORT and http://HOST:PORT
- Save this list to:
	- temp/agent1/web_port_targets.txt

Then run httpx against temp/agent1/web_port_targets.txt to identify which host:port combos are actually speaking HTTP.
- Output your cleaned live list to:
	- temp/agent1/live_seeds.txt

Note:
- Port 9100 is commonly Prometheus/node_exporter metrics, which is HTTP and often sensitive.
- Treat any non-standard-port HTTP service as high priority for manual review.

Optional validation:
- If you find high-value ports/services, validate with nmap for accuracy.

Step 0B: TLS/Certificate intel (Recommended)
Tool: tlsx

Instruction:
- Run tlsx on hosts from outputs/activesubdomain.txt (or on web targets derived from ports).
- Use it to extract certificate SANs/subjects and spot extra hostnames.
- Any new hostnames must still pass scope rules before adding to Step 1/2 inputs.

Outputs:
- temp/agent1/tlsx.txt
- outputs/coverage_tls_sans_in_scope.txt (any in-scope extra hostnames you can prove are in scope)

Step 0C: Screenshot triage (Recommended)
Tool: gowitness

Instruction:
- Screenshot the web targets (from temp/agent1/live_seeds.txt once created, and/or from outputs/coverage_open_ports_hostport.txt filtered to web ports).
- Use this to quickly spot admin panels, staging apps, and distinct products.

Troubleshooting (common failure: Chrome/Chromium missing)
- gowitness typically needs a local Chrome/Chromium binary.
- If you see an error like: `exec: "google-chrome": executable file not found in $PATH`:
	- Install Chrome/Chromium (if your environment allows).
	- Or configure gowitness to use the correct browser path (flag name varies; check `gowitness -h`).
	- If you cannot install a browser (locked environment), skip screenshots and record it in outputs/reports/agent1-error-report.md.

Outputs:
- temp/agent1/screenshots/
- outputs/coverage_screenshots_index_gowitness.txt

Step 0D: Subdomain takeover candidates (Scope-aware)
Tool: subjack

Instruction:
- Run subjack on outputs/activesubdomain.txt to identify potential takeover indicators.
- Treat results as candidates only; manual verification must follow program rules.

Outputs:
- temp/agent1/takeover_candidates_raw.txt
- outputs/coverage_takeover_candidates_subjack.txt

Step 0E: CNAME verification for takeover risk (Recommended but high value)
Why:
- "store" / "shop" / "cdn" / "assets" subdomains often point to third-party platforms (Shopify/BigCommerce/etc).
- A dangling CNAME can lead to subdomain takeover.

Instruction:
- Collect CNAME targets for all in-scope subdomains:
	- Output: temp/agent1/cnames.txt
- Prioritize manual review for any subdomain whose CNAME points to third-party services.
- Cross-check with takeover candidates from subjack.

Output:
- temp/agent1/cnames.txt

Step A: The Filter (Probing) (Fallback only)
Tool: httpx

Logic: Do not scan dead domains.

Primary path:
- Use Step 0A1 output (temp/agent1/live_seeds.txt) when you have open ports / web_port_targets.

Fallback path (only if Step 0A/0A1 was skipped or produced no usable web targets):
- Probe default ports (80/443) from outputs/activesubdomain.txt to create temp/agent1/live_seeds.txt.

AI Decision:
- If a host returns 403/401, do not discard it; treat it as live.

Brain Stop #2 (Mandatory): Post-Probe Decisions
Instruction to Jules:
- STOP running tools.
- Review temp/agent1/live_seeds.txt and quickly sanity-check:
	- If temp/agent1/live_seeds.txt is empty: STOP. Do not continue Step 2.
	  Re-check Step 1 output quality, scope, DNS resolution, and httpx flags/ports.
	- Are many hosts returning 429/503/WAF pages?
	- Any unexpected out-of-scope redirects?
- Decide safe scan settings for later steps (rate/parallelism) based on this.
- Append these decisions to: outputs/reports/agent1-recon-report.md

Step A1: Fingerprinting (Mandatory)
Tool: httpx

Goal:
- Capture quick fingerprints per live host to guide later testing (stack, server, redirects, WAF/CDN hints).

Instruction:
- Run httpx against temp/agent1/live_seeds.txt and write a fingerprint report.
- Prefer enabling: status code, title, redirect location, IP, web-server header, tech detect.
- Flag names vary by httpx version; choose the closest supported flags.

Output:
- temp/agent1/host_fingerprints.txt

Step A1b: WAF Detection (Mandatory)
Tool: wafw00f

Goal:
- Detect WAF/CDN protections early so later heavy tools (Katana/Ffuf/Naabu) do not get you blocked and produce false negatives.

Instruction:

1) Prepare clean base URLs (recommended)
- wafw00f behaves best when you feed it base URLs (scheme + host) rather than long paths.
- Create a base-url list from your live seeds:
	- Input: temp/agent1/live_seeds.txt
	- Output: temp/agent1/live_base_urls.txt

2) Run wafw00f
- Run wafw00f against temp/agent1/live_base_urls.txt and save results to:
	- temp/agent1/waf_results.txt

Suggested operational pattern (version-dependent; verify with `wafw00f -h`):
- Prefer file input mode (if supported) and write output to a file.
- If file mode is not supported, loop base URLs one-by-one and append results.

3) Handle common errors (do not guess)
- If you see SSL/handshake errors on a host:
	- Retry that host with http:// instead of https:// (some subdomains are misconfigured).
	- If it still fails, mark that host as "WAF inconclusive" (do not assume "no WAF").
- If you see timeouts:
	- Re-run wafw00f for ONLY the failing hosts with a higher timeout / lower parallelism.
	- If it still fails, treat it as "possibly protected / unstable" and scan gently.

Important note:
- A host that times out/refuses connections is NOT proof of WAF.
- Treat it as: "blocked/unstable/inconclusive" and adjust scan rates accordingly.

Interpretation rules (important):
- Treat WAF detection as a signal to adjust speed, not a reason to stop recon.
- If many hosts are WAF-protected, you MUST lower aggressiveness for: Katana, Ffuf, Naabu.
- Always log: "WAF detected / inconclusive / not detected" per host in your report.

Output:
- temp/agent1/waf_results.txt

Brain Stop #3 (Mandatory): Fingerprint-Based Prioritization
Instruction to Jules:
- STOP running tools.
- Review temp/agent1/host_fingerprints.txt and identify:
	- Likely real apps vs parked/placeholder hosts
	- API-looking hosts/paths (e.g., api., gateway., /api)
	- WAF/CDN hints and rate-limit behavior
- Use this to prioritize crawling/bruteforcing effort.
- ALSO review temp/agent1/waf_results.txt:
	- If WAF/CDN is detected on many hosts, reduce aggressiveness for subsequent steps.
	  Apply safer settings such as: lower concurrency, add delays, and/or per-host rate-limits.
	  (Exact flags differ by tool version; choose the closest supported options.)
- Append your prioritized host list + why to: outputs/reports/agent1-recon-report.md

Step A2: HTML Extractor (Save HTML for offline analysis)
Tool: httpx

Goal:
- Save the actual HTML responses from live targets for later manual review/diffing.

Instruction:
- Fetch live pages from temp/agent1/live_seeds.txt and store responses to temp/agent1/html_responses/
- Prefer httpx response storage flags (names vary by version):
	- `-sr` / `-store-response`
	- `-srd temp/agent1/html_responses` / `-store-response-dir temp/agent1/html_responses`

Output:
- temp/agent1/html_responses/*

Step A3: Known Files + API Docs Discovery (Mandatory)
Tools: httpx + ffuf

Goal:
- Quickly find robots/sitemaps/security.txt and API documentation endpoints (Swagger/OpenAPI).

Instruction:
- Use temp/agent1/knownfiles_and_apidocs.txt as the wordlist.
- For each live host, request https://HOST/FUZZ and/or http://HOST/FUZZ (keep in-scope only).
- Store hits and split them into:
	- robots/sitemap/security findings
	- api docs findings (swagger/openapi)

Outputs:
- temp/agent1/known_files_hits.txt
- outputs/web_knownfiles_robots_sitemaps_security_urls.txt
- outputs/api_docs_urls.txt

Step A3b: Parse Swagger/OpenAPI docs into endpoints (Mandatory)
Why:
- Swagger/OpenAPI files are a "map" of hidden endpoints.
- These endpoints are often not linked anywhere else and are high-value for IDOR/SQLi/auth bugs.

Inputs:
- outputs/api_docs_urls.txt (URLs to swagger/openapi docs)
- outputs/activesubdomain.txt (allowlist enforcement)

Instruction:
- Download the API docs for offline analysis:
	- Save raw files under: temp/agent1/api_docs_raw/
- Parse the JSON docs and extract endpoints into a clean, in-scope URL list:
	- outputs/api_endpoints_from_openapi.txt

Implementation option (recommended):
- Use the extractor script: tools/agent1/assets/openapi_extractor.py
	- It reads outputs/api_docs_urls.txt, enforces the allowlist, and writes outputs/api_endpoints_from_openapi.txt.

After this step:
- Merge outputs/api_endpoints_from_openapi.txt into the main URL corpus during Step F.

Brain Stop #4 (Mandatory): API Docs / Sitemap Leverage
Instruction to Jules:
- STOP running tools.
- If outputs/api_docs_urls.txt contains Swagger/OpenAPI docs, treat them as high-truth.
- If robots/sitemaps reveal new paths, ensure they are added into the URL pool for merge.
- Decide what basepaths should be prioritized in Step 3 (e.g., /api, /v1).
- Append a short “API docs leverage” note to: outputs/reports/agent1-recon-report.md

Step B: History Mining (The Time Machine)
Tools: Waymore + Gau

Logic: Developers often delete vulnerable pages but forget to remove them from archives.

Instruction: Run Waymore for deeper archive pulls. Run Gau for broader pulls.
Note: If you want to focus on recent history (e.g., last 2 years), do it via tool options if supported; otherwise post-filter by date.

Output: Save to temp/agent1/raw_history.txt.

Step C: Parameter Hunting (The Miner)
Tool: ParamSpider

Logic: We specifically want URLs that accept input (parameters), as these are attackable.

Instruction: Run ParamSpider on the domains to extract parameterized URLs.
Note: If ParamSpider does not support list input in your version, loop domains one-by-one.

Step D: Active Crawling (The Spider)
Tools: Katana + Hakrawler

Logic: Visit the live sites now.

AI Decision: Use Katana with JavaScript enabled (-jc). If Katana gets stuck or takes too long on a specific domain (e.g., > 10 mins), kill it and switch to Hakrawler for that domain.

Step E: Source Code Recon (The Leak Hunter)
Tool: GitDorker (Requires GitHub Token)

Logic: Developers accidentally push internal links to GitHub.

Instruction: Search GitHub for the domain name + keywords like "api", "secret", "admin".

AI Note: If you don't have a GitHub token configured, skip this tool, but note in the logs that Source Code recon was skipped.

Step F: Merge + Dedupe (single corpus for analysis)
Logic:
- Merge outputs from: history, param mining, crawling, GitDorker, known-files/api-docs hits
- ALSO merge: outputs/api_endpoints_from_openapi.txt (from Step A3b)
- Remove duplicates (use anew if available)
- Keep only in-scope URLs (enforce the allowlist: host must be in outputs/activesubdomain.txt)

Recommended implementation detail:
- Extract host with unfurl when enforcing allowlist
- Run uro after merge (then dedupe again) to heavily reduce duplicates

Output:
- temp/agent1/url_corpus_all_in_scope.txt (deduped merged list)

Brain Stop #5 (Mandatory): Final URL Corpus Quality Gate
Instruction to Jules:
- STOP running tools.
- Verify temp/agent1/url_corpus_all_in_scope.txt is:
	- In-scope only (host allowlist enforced)
	- Normalized (uro applied)
	- Duplicates minimized
- If out-of-scope URLs exist, fix allowlist filtering and re-run merge.
- Append corpus stats (line counts) + any filtering issues to: outputs/reports/agent1-recon-report.md

Step 2 complete
- This is the end of Step 2 (collection + artifacts + merge).
- Proceed to Step 3 for analysis + prioritized heavy scans + bucketing:
	- tools/agent1/step3-analysis-and-prioritized-scanning.txt
