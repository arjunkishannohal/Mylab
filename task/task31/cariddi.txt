# Tool 31 — Cariddi (optional enrichment) — STRICT RUN-CARD
# Goal: crawl from live seeds and hunt for secrets/errors/info/endpoints.
# This is OPTIONAL. Use it to find things katana/gau/js analysis might miss.
#
# Input (contract)
# - outputs/live_base_urls.txt        (from Task 8)
# - outputs/activesubdomain.txt       (scope allowlist)
#
# Outputs (contract)
# - outputs/cariddi/cariddi_findings.txt        (raw findings from cariddi stdout)
# - outputs/cariddi/cariddi_urls_raw.txt        (URLs extracted via regex from findings)
# - outputs/cariddi/cariddi_urls_in_scope.txt   (allowlist-filtered URLs)
#
# Notes
# - Keep it scope-safe: DO NOT use -intensive unless you accept scope expansion.
# - Determinism: we always sort+dedupe outputs.
# - License note: cariddi is GPL-3.0; install it externally (don’t vendor).
#
# ----------------------------
# 1) Install
# ----------------------------
# Requires Go. Install cariddi:
#   go install -v github.com/edoardottt/cariddi/cmd/cariddi@latest
# Verify:
#   cariddi -version

# ----------------------------
# 2) Preconditions
# ----------------------------
# Must have live seeds:
#   if (!(Test-Path outputs\live_base_urls.txt)) { throw "Missing outputs\\live_base_urls.txt (run Task 8 first)" }
# Allowlist must exist:
#   if (!(Test-Path outputs\activesubdomain.txt)) { throw "Missing outputs\\activesubdomain.txt" }

# ----------------------------
# 3) Run Cariddi (PowerShell; from repo root)
# ----------------------------
New-Item -ItemType Directory -Force outputs\cariddi | Out-Null

# Recommended baseline (tune concurrency/depth to stay under 9 minutes)
# -e    : endpoints hunting
# -s    : secrets hunting (regex-based; expect FP/FN)
# -err  : error hunting
# -info : useful info hunting
# -md 2 : max depth 2 (keep bounded)
# -c 30 : concurrency
# -t 10 : timeout seconds
# -plain: keep output easy to parse

Get-Content outputs\live_base_urls.txt |
  cariddi -e -s -err -info -md 2 -c 30 -t 10 -plain |
  Tee-Object -FilePath outputs\cariddi\cariddi_findings.txt

# ----------------------------
# 4) Extract URLs from findings (clean + deterministic)
# ----------------------------
# Cariddi output is not guaranteed to be only URLs; extract any http(s)://... strings.
Select-String -Path outputs\cariddi\cariddi_findings.txt -Pattern 'https?://[^\s"''<>]+' -AllMatches |
  ForEach-Object { $_.Matches.Value } |
  ForEach-Object { $_.Trim().TrimEnd('.', ',', ';') } |
  Where-Object { $_ } |
  Sort-Object -Unique |
  Set-Content -Encoding utf8 outputs\cariddi\cariddi_urls_raw.txt

# ----------------------------
# 5) Scope-filter the extracted URLs
# ----------------------------
# Uses your existing allowlist filter script.
python task\task21\allowlist_filter_urls.py --allowlist outputs\activesubdomain.txt --in outputs\cariddi\cariddi_urls_raw.txt --out outputs\cariddi\cariddi_urls_in_scope.txt

# ----------------------------
# 6) Optional: merge into the pipeline (RECOMMENDED WAY)
# ----------------------------
# Do NOT overwrite other canonical outputs.
# Instead, create/extend an aggregated URL corpus file (optional, used by later tasks).
#
# If you already use outputs/url_corpus_all_in_scope.txt (from Task 15/16 allowlist filter), append cariddi:
#   if (Test-Path outputs\url_corpus_all_in_scope.txt) {
#     Get-Content outputs\url_corpus_all_in_scope.txt, outputs\cariddi\cariddi_urls_in_scope.txt |
#       Sort-Object -Unique |
#       Set-Content -Encoding utf8 outputs\url_corpus_all_in_scope.txt
#   } else {
#     Copy-Item outputs\cariddi\cariddi_urls_in_scope.txt outputs\url_corpus_all_in_scope.txt
#   }
#
# Then you can feed outputs/url_corpus_all_in_scope.txt into:
# - Task 16/17 arjun
# - httpx probe (URL list mode)
# - nuclei / ffuf (after probing)

# ----------------------------
# 7) Output sanity check
# ----------------------------
#   if (!(Test-Path outputs\cariddi\cariddi_findings.txt)) { throw "Missing outputs\\cariddi\\cariddi_findings.txt" }
#   if (!(Test-Path outputs\cariddi\cariddi_urls_in_scope.txt)) { throw "Missing outputs\\cariddi\\cariddi_urls_in_scope.txt" }
