================================================================================
TASK 104 - VULNERABILITY CONSOLIDATION
================================================================================
Covers testing_toolkit.txt Phase 16 Step 59
Consolidate all findings, deduplicate, verify, assign severity

TIME TO ORGANIZE THE CHAOS
All those vulnerability reports need to be merged, deduplicated, verified.

================================================================================
INPUTS
================================================================================
outputs/vulnerabilities/*.md               <- All vulnerability reports
outputs/*/findings.json                    <- JSON findings from all phases
temp/agent1/*_results.txt                  <- Raw results files

================================================================================
OUTPUTS
================================================================================
outputs/reports/
    master_vulnerability_list.json         <- All vulns, deduplicated
    vulnerability_summary.md               <- Executive summary
    by_severity/
        CRITICAL.md
        HIGH.md
        MEDIUM.md
        LOW.md
        INFO.md

================================================================================
ðŸ§  AGENT DECISION FRAMEWORK
================================================================================

CONSOLIDATION WORKFLOW:

    Collect all findings:
    |
    +-- Vulnerability reports (*.md)
    |   +-- Parse each report
    |   +-- Extract URL, type, severity
    |
    +-- JSON findings
    |   +-- Merge all JSON files
    |   +-- Normalize format
    |
    +-- Raw results
        +-- Parse for additional issues
        +-- Missed during initial triage

    Deduplicate:
    |
    +-- By URL + vulnerability type
    |   +-- Same URL + same vuln = duplicate
    |   +-- Keep highest severity
    |
    +-- By root cause
        +-- Multiple URLs, same root cause
        +-- Group as single finding

    Verify:
    |
    +-- Re-test critical/high findings
    +-- Ensure exploitability
    +-- Update false positives

    Assign final severity:
    |
    +-- CVSS scoring
    +-- Business impact
    +-- Exploitability

================================================================================
PHASE 1: COLLECT ALL FINDINGS
================================================================================

-----------------------------------------
1.1 Parse Vulnerability Reports
-----------------------------------------
#!/usr/bin/env python3
"""
vulnerability_consolidator.py - Consolidate all vulnerability findings
"""

import os
import json
import re
import glob
from collections import defaultdict
from datetime import datetime

os.makedirs('outputs/reports', exist_ok=True)
os.makedirs('outputs/reports/by_severity', exist_ok=True)

class VulnerabilityConsolidator:
    
    def __init__(self):
        self.findings = []
        self.master_list = []
    
    def parse_markdown_report(self, filepath):
        """Parse a markdown vulnerability report"""
        
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        finding = {
            'source_file': filepath,
            'raw_content': content
        }
        
        # Extract URL
        url_match = re.search(r'\*\*URL\*\*:?\s*([^\n]+)', content)
        if url_match:
            finding['url'] = url_match.group(1).strip()
        
        # Extract severity from filename or content
        severity_match = re.search(r'(CRITICAL|HIGH|MEDIUM|LOW|INFO)', filepath.upper())
        if severity_match:
            finding['severity'] = severity_match.group(1)
        else:
            sev_content = re.search(r'\*\*Severity\*\*:?\s*([^\n]+)', content)
            if sev_content:
                finding['severity'] = sev_content.group(1).strip().upper()
            else:
                finding['severity'] = 'MEDIUM'
        
        # Extract vulnerability type from filename
        filename = os.path.basename(filepath)
        type_match = re.search(r'^([A-Z0-9-]+)', filename)
        if type_match:
            finding['type'] = type_match.group(1)
        
        # Extract title
        title_match = re.search(r'^#\s+([^\n]+)', content)
        if title_match:
            finding['title'] = title_match.group(1).strip()
        
        # Extract description
        desc_match = re.search(r'##\s*Description\s*\n([^#]+)', content)
        if desc_match:
            finding['description'] = desc_match.group(1).strip()[:500]
        
        # Extract PoC
        poc_match = re.search(r'```[^\n]*\n([^`]+)```', content)
        if poc_match:
            finding['poc'] = poc_match.group(1).strip()
        
        return finding
    
    def parse_json_findings(self, filepath):
        """Parse JSON findings file"""
        
        try:
            with open(filepath, 'r') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                for item in data:
                    item['source_file'] = filepath
                return data
            elif isinstance(data, dict):
                data['source_file'] = filepath
                return [data]
                
        except:
            return []
    
    def collect_all_findings(self):
        """Collect findings from all sources"""
        
        print("[*] Collecting vulnerability reports...")
        
        # Markdown reports
        md_files = glob.glob('outputs/vulnerabilities/*.md')
        for md_file in md_files:
            finding = self.parse_markdown_report(md_file)
            if finding.get('url'):
                self.findings.append(finding)
        
        print(f"    Found {len(md_files)} markdown reports")
        
        # JSON findings
        json_files = glob.glob('outputs/*/findings.json')
        json_files += glob.glob('outputs/*/*.json')
        
        for json_file in json_files:
            if 'master' not in json_file.lower():
                findings = self.parse_json_findings(json_file)
                self.findings.extend(findings)
        
        print(f"    Found {len(json_files)} JSON files")
        print(f"    Total raw findings: {len(self.findings)}")

================================================================================
PHASE 2: DEDUPLICATE
================================================================================

-----------------------------------------
2.1 Deduplication Logic
-----------------------------------------
    def generate_fingerprint(self, finding):
        """Generate unique fingerprint for deduplication"""
        
        from urllib.parse import urlparse
        
        url = finding.get('url', '')
        vuln_type = finding.get('type', finding.get('vulnerability', ''))
        
        # Normalize URL
        if url:
            parsed = urlparse(url)
            # Remove query string variations
            normalized_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        else:
            normalized_url = ''
        
        # Normalize type
        vuln_type = str(vuln_type).upper().replace('-', '_')
        
        return f"{normalized_url}|{vuln_type}"
    
    def deduplicate(self):
        """Remove duplicate findings"""
        
        print("[*] Deduplicating findings...")
        
        fingerprints = {}
        severity_rank = {'CRITICAL': 5, 'HIGH': 4, 'MEDIUM': 3, 'LOW': 2, 'INFO': 1}
        
        for finding in self.findings:
            fp = self.generate_fingerprint(finding)
            
            if fp in fingerprints:
                # Keep higher severity
                existing = fingerprints[fp]
                existing_rank = severity_rank.get(existing.get('severity', 'INFO'), 0)
                new_rank = severity_rank.get(finding.get('severity', 'INFO'), 0)
                
                if new_rank > existing_rank:
                    fingerprints[fp] = finding
            else:
                fingerprints[fp] = finding
        
        self.master_list = list(fingerprints.values())
        
        print(f"    Deduplicated: {len(self.findings)} -> {len(self.master_list)}")

================================================================================
PHASE 3: NORMALIZE AND ENRICH
================================================================================

-----------------------------------------
3.1 Normalize Findings
-----------------------------------------
    def normalize_findings(self):
        """Normalize all findings to standard format"""
        
        print("[*] Normalizing findings...")
        
        normalized = []
        
        for i, finding in enumerate(self.master_list):
            norm = {
                'id': f"VULN-{i+1:04d}",
                'url': finding.get('url', finding.get('endpoint', 'Unknown')),
                'type': finding.get('type', finding.get('vulnerability', 'Unknown')),
                'severity': finding.get('severity', 'MEDIUM').upper(),
                'title': finding.get('title', f"{finding.get('type', 'Unknown')} Vulnerability"),
                'description': finding.get('description', ''),
                'poc': finding.get('poc', ''),
                'source': finding.get('source_file', ''),
                'verified': False,
                'false_positive': False,
                'timestamp': datetime.now().isoformat()
            }
            
            # Ensure severity is valid
            if norm['severity'] not in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'INFO']:
                norm['severity'] = 'MEDIUM'
            
            normalized.append(norm)
        
        self.master_list = normalized

================================================================================
PHASE 4: GENERATE REPORTS
================================================================================

-----------------------------------------
4.1 Executive Summary
-----------------------------------------
    def generate_summary(self):
        """Generate executive summary"""
        
        print("[*] Generating executive summary...")
        
        # Count by severity
        severity_counts = defaultdict(int)
        for finding in self.master_list:
            severity_counts[finding['severity']] += 1
        
        # Count by type
        type_counts = defaultdict(int)
        for finding in self.master_list:
            type_counts[finding['type']] += 1
        
        summary = f"""# Vulnerability Assessment Summary

**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Total Findings**: {len(self.master_list)}

## Severity Distribution

| Severity | Count |
|----------|-------|
| CRITICAL | {severity_counts.get('CRITICAL', 0)} |
| HIGH | {severity_counts.get('HIGH', 0)} |
| MEDIUM | {severity_counts.get('MEDIUM', 0)} |
| LOW | {severity_counts.get('LOW', 0)} |
| INFO | {severity_counts.get('INFO', 0)} |

## Top Vulnerability Types

| Type | Count |
|------|-------|
"""
        
        for vuln_type, count in sorted(type_counts.items(), key=lambda x: -x[1])[:15]:
            summary += f"| {vuln_type} | {count} |\n"
        
        summary += """

## Risk Assessment

"""
        
        if severity_counts.get('CRITICAL', 0) > 0:
            summary += "âš ï¸ **CRITICAL vulnerabilities found - immediate action required**\n\n"
        
        if severity_counts.get('HIGH', 0) > 0:
            summary += "âš ï¸ **HIGH severity issues require prompt attention**\n\n"
        
        summary += """
## Recommendations

1. Address CRITICAL vulnerabilities immediately
2. Plan remediation for HIGH severity issues within 7 days
3. Schedule MEDIUM issues for next sprint
4. LOW/INFO can be addressed during regular maintenance

---

*See detailed findings in master_vulnerability_list.json*
"""
        
        with open('outputs/reports/vulnerability_summary.md', 'w') as f:
            f.write(summary)

-----------------------------------------
4.2 Severity-Based Reports
-----------------------------------------
    def generate_severity_reports(self):
        """Generate reports grouped by severity"""
        
        print("[*] Generating severity-based reports...")
        
        by_severity = defaultdict(list)
        for finding in self.master_list:
            by_severity[finding['severity']].append(finding)
        
        for severity, findings in by_severity.items():
            report = f"# {severity} Severity Vulnerabilities\n\n"
            report += f"**Count**: {len(findings)}\n\n"
            report += "---\n\n"
            
            for finding in findings:
                report += f"## {finding['id']}: {finding['title']}\n\n"
                report += f"**URL**: {finding['url']}\n"
                report += f"**Type**: {finding['type']}\n\n"
                
                if finding['description']:
                    report += f"### Description\n{finding['description']}\n\n"
                
                if finding['poc']:
                    report += f"### PoC\n```\n{finding['poc']}\n```\n\n"
                
                report += "---\n\n"
            
            with open(f"outputs/reports/by_severity/{severity}.md", 'w') as f:
                f.write(report)

================================================================================
PHASE 5: FULL AUTOMATION
================================================================================

#!/usr/bin/env python3
"""
consolidate_vulnerabilities.py - Full consolidation workflow
"""

import os
import json
import re
import glob
from collections import defaultdict
from datetime import datetime

os.makedirs('outputs/reports', exist_ok=True)
os.makedirs('outputs/reports/by_severity', exist_ok=True)

class VulnConsolidator:
    
    def __init__(self):
        self.findings = []
        self.master = []
    
    def collect(self):
        """Collect all findings"""
        
        # Markdown
        for f in glob.glob('outputs/vulnerabilities/*.md'):
            finding = self._parse_md(f)
            if finding:
                self.findings.append(finding)
        
        # JSON
        for f in glob.glob('outputs/**/*.json', recursive=True):
            if 'master' not in f:
                data = self._parse_json(f)
                self.findings.extend(data)
    
    def _parse_md(self, path):
        try:
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            url = re.search(r'\*\*URL\*\*:?\s*([^\n]+)', content)
            sev = re.search(r'(CRITICAL|HIGH|MEDIUM|LOW|INFO)', path.upper())
            title = re.search(r'^#\s+([^\n]+)', content)
            
            return {
                'url': url.group(1).strip() if url else '',
                'severity': sev.group(1) if sev else 'MEDIUM',
                'title': title.group(1).strip() if title else os.path.basename(path),
                'type': os.path.basename(path).split('-')[0],
                'source': path
            }
        except:
            return None
    
    def _parse_json(self, path):
        try:
            with open(path) as f:
                data = json.load(f)
            
            results = []
            items = data if isinstance(data, list) else [data]
            
            for item in items:
                if isinstance(item, dict) and (item.get('url') or item.get('endpoint')):
                    item['source'] = path
                    results.append(item)
            
            return results
        except:
            return []
    
    def dedupe(self):
        """Deduplicate"""
        seen = {}
        sev_rank = {'CRITICAL': 5, 'HIGH': 4, 'MEDIUM': 3, 'LOW': 2, 'INFO': 1}
        
        for f in self.findings:
            key = f"{f.get('url', '')}|{f.get('type', '')}"
            
            if key in seen:
                if sev_rank.get(f.get('severity', 'INFO'), 0) > \
                   sev_rank.get(seen[key].get('severity', 'INFO'), 0):
                    seen[key] = f
            else:
                seen[key] = f
        
        self.master = list(seen.values())
    
    def save(self):
        """Save results"""
        
        # Master list
        with open('outputs/reports/master_vulnerability_list.json', 'w') as f:
            json.dump(self.master, f, indent=2, default=str)
        
        # Summary
        sev_counts = defaultdict(int)
        for f in self.master:
            sev_counts[f.get('severity', 'MEDIUM')] += 1
        
        summary = f"""# Vulnerability Summary

**Total**: {len(self.master)}

| Severity | Count |
|----------|-------|
| CRITICAL | {sev_counts.get('CRITICAL', 0)} |
| HIGH | {sev_counts.get('HIGH', 0)} |
| MEDIUM | {sev_counts.get('MEDIUM', 0)} |
| LOW | {sev_counts.get('LOW', 0)} |
| INFO | {sev_counts.get('INFO', 0)} |
"""
        
        with open('outputs/reports/vulnerability_summary.md', 'w') as f:
            f.write(summary)
        
        # By severity
        by_sev = defaultdict(list)
        for f in self.master:
            by_sev[f.get('severity', 'MEDIUM')].append(f)
        
        for sev, findings in by_sev.items():
            report = f"# {sev} Vulnerabilities ({len(findings)})\n\n"
            for i, f in enumerate(findings):
                report += f"## {i+1}. {f.get('title', f.get('type', 'Unknown'))}\n"
                report += f"**URL**: {f.get('url', 'N/A')}\n\n"
            
            with open(f'outputs/reports/by_severity/{sev}.md', 'w') as out:
                out.write(report)

if __name__ == "__main__":
    c = VulnConsolidator()
    c.collect()
    print(f"[*] Collected {len(c.findings)} findings")
    c.dedupe()
    print(f"[*] After dedup: {len(c.master)}")
    c.save()
    print("[*] Reports saved to outputs/reports/")

================================================================================
SUMMARY CHECKLIST
================================================================================

[ ] All vulnerability reports collected
[ ] All JSON findings collected
[ ] Duplicates removed
[ ] Findings normalized
[ ] Executive summary generated
[ ] Severity-based reports generated
[ ] Master list saved as JSON

================================================================================
NEXT TASK
================================================================================
Task 105: Final Report Generation (Phase 16 - Closeout)
